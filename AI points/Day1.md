
## 1.  为什么要对特征做归一化

为了使不同指标之间具有可比性，消除数据特征之间的量纲影响。

**例子：** 分析身高和体重对健康的影响，BMI指数。身高：统一为米(m)，体重：统一为千克(kg)。BMI = 体重 / (身高)^2。

想要得到更为精确的结果，就需要进行特征归一化处理(normalization)，使个指标处于同一数值量级，以便进行分析。


## 2.  什么是组合特征？如何处理高位组合特征？
    
为了提高复杂关系的拟合能力，特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

组合特征：指两个或多个特征相乘形成的合成特征。特征的想和组合可以提供超出这些特征单独能够提供的预测能力。

    例子：
    广告点击预估问题。
    原始数据：语言 + 类型，两种离散特征。
    二阶特征 = 语言 * 类型， 求二阶特征对广告点击的影响， 会提高单独分析 语言/类型 对广告点击的影响的预测能力。

在互联网环境下，规模可达到千万量级，几乎无法学习 m*n 规模的参数。

如何处理高位组合特征：在这种情况下，可进行 k 维的低维向量表示。

m*n 变成 m*k + n*k，其实等价于矩阵分解。


## 3.  请比较欧氏距离与曼哈顿距离？

    欧氏距离：dis = √(x1 - y1)^2 + (x2 - y2)^2 + ... +(xn - yn)^2

    曼哈顿距离：dis = ｜x1 - x2｜+|y1 - y2|

欧氏距离：是两点之间的直线距离。

曼哈顿距离：城市街区中的驾驶距离。直角坐标系上亮点所形成的线段对轴产生的投影的距离总和。


## 4.  为什么一些场景中使用余弦相似度而不是欧氏距离？

余弦相似度：  通过测量两个向量的夹角的余弦值来评估两者之间的相似性。

            余弦值 >> 1, 夹角 >> 0, 两个向量越相似。

            余弦值 >> 0, 夹角 >> 90度, 两个向量越不相似。

用余弦相似度计算两段文本的相似度

    过程：

    1.  分词
    2.  在set()中列出所有词
    3.  分词编码：将每个字转换为出现在set中的位置
    4.  One-hot 编码：计算每个分析出现的次数，得到两个句子的词频向量。
    5.  余弦相似度：计算两个词频向量之间夹角的余弦值，判断两个句子相似度。

欧氏距离：体现数值上的绝对差异。

余弦距离：体现方向上的绝对差异。

哪些场景中使用余弦相似度而不是欧氏距离：

**关键：**   欧氏距离给予了每个方向同样的重视，缺乏先验。而在高维中，如果有信号稀疏的先验，可以通过稀疏信号移除非稀疏信号。

    所以在文本聚类里，对于稀疏向量，一般用余弦相似度要好过欧氏距离。

(https://stats.stackexchange.com/questions/29627/euclidean-distance-is-usually-not-good-for-sparse-data-and-more-general-case)

    
## 5.  One-hot 的作用是什么？为什么不直接使用数字作为表示？

One-hot编码：One-hot编码可以将 **类别性数据** 转换成统一的 **数字格式**。

方法：使用 N 位状态寄存器来对 N 个状态进行编码，每个状态都有独立的寄存器位，在任意时候，只有一位有效位。

    例子：
    特征“好，中，差” --> One-hot “100, 010, 001”。
    特征之间相互互斥，每次只有一个激活，因为数据是稀疏的。

好处：

    1. 解决了分类器不好处理 类别性数据 的问题。
    2. 解决 类别性数据 离散值问题。
    3. 一定程度上起到了扩充特征的作用。

为什么要用 One-hot编码：离散性特征使用One-hot编码，会让特征之间的距离计算更加合理。

为什么不直接用数字作为表示：直接使用数字会给将 人工误差 而导致的假设引入 到类别特征中。

## 6.  在模型评估过程中，过拟合和欠拟合具体指什么现象？

过拟合(overfitting): 指算法模型在训练集上的性能非常好，但是泛化能力很差，泛化误差很大。一般是由于训练数据少和噪声以及模型能力强等原因造成的。

欠拟合(underfitting): 模型不能很好的拟合训练数据，在训练集的错误率比较高。一般是由于模型能力不足造成的。


## 7.  降低过拟合和欠拟合的方法？

降低过拟合：

    1.  一般在经验风险最小化的基础上再引入参数的正则化，来限制模型能力。
    2.  降低模型复杂度。
    3.  集成学习：通过把多个模型集成在一起，来降低单个模型的过拟合风险。
    4.  获取更多的训练数据：因为更多的样本可以使得模型学习到更多的有效特征，从而减小噪声的影响，提升泛化性能。

降低欠拟合：

    1.  添加新特征：使训练数据具有更丰富的特征。
    2.  增加模型复杂度：简单模型的学习能力较弱，通过增加模型的复杂度可以使模型具有更强的拟合能力。
    3.  减少正则化系数：正则化是用来防止过拟合的，但当模型出现欠拟合时，应当有针对性的减少正则化系数。
