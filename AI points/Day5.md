**关键词：**

    - 常用聚类算法：K-means
    - SVM算法简介

聚类(cluster)：

    按照某个特定标准 (如：距离)，把一个数据集分割成不同的类or簇，使得同一个簇内的数据对象的相似性尽可能的大，同时不在同一个簇中的数据对象的差异性也尽可能的大。

    聚类后的同一类的数据尽可能聚集到一起，不同类数据尽量分离。

    聚类是一种无监督学习方法。

    一般过程：

        1.  数据准备：  特征标准化和降维。
        2.  特征选择：  从最初的特征中选择最有效的特征，并将其存储在向量中。
        3.  特征提取：  通过对选择的特征进行转换形成新的突出特征。
        4.  聚类：  基于距离函数进行相似度度量，获得簇。
        5.  聚类结果评估：  分析聚类结果， 如 距离误差 和(SSE)等。

    聚类的相似度度量方法：
        数值型数据的相似度度量方法：

            1.  Euclidean距离
            2.  Manhattan距离
            3.  Chebyshev距离
            4.  Minkowski距离

    聚类方法：

        1.  划分式聚类方法：    需要实现制定簇类的数目or聚类中心，通过反复迭代，直至最后达到目标：“簇内的点足够近，簇间的点足够远”。

            - K-means
            - K-means++
            - Bi-kmeans

        2.  基于密度的聚类方法
        3.  层次化聚类方法
        4.  新方法


1.  K-means 流程
====

K-means (K-means clustering algorithm)算法流程：

    1.  创建 k 个点作为初始质心(通常是 随机选择)。
    2.  当任意一个点的簇分配结果发生改变时：
        - 对数据集中的每个数据点
            - 对每个质心
                - 计算 质心 与 数据点之间的距离
           - 将数据点分配到距其最近的簇
        - 对每个簇，计算簇中所有点的均值并将均值作为新质心
    3.  重复 2 ，直到中心点变化小于筏值or不再变化

K-means 算法特点：

    1.  需要提前确定 k 值。
    2.  对初始质心点敏感
    3.  对异常数据敏感
    4.  抗噪点能力较差
    5.  算法效率很高

2.  K-means 对异常值是否敏感，为什么？
====

k-means 对异常值较为敏感，因为一个集合内的元素均值容易受到一个**极大值**的影响。
当存在异常值时，均值计算出来的**中心位置**很可能不能够反映**真实**的类别中心。

3.  如何评估聚类效果
====

1.  聚类趋势：对数据进行评估
    Hopkins Statistic 评估给定数据集 是否存在 有意义的**可聚类的非随机结构**。
    如果一个数据集是有随机的均匀的点生成的，虽然也可以产生聚类效果，但是没有意义。
    聚类的前提是：需要数据**非均匀分布**。

2.  判断聚类的簇数是否为最佳
    业务分析法
    观察法
    Gap Statistic方法
    找到最佳的分类数与实际簇数做比较。
    (超参数 k 如何选择)

3.  聚类质量
    因为 K-means 是无监督学习，所以一般通过评估类的分离情况来决定聚类质量。
    类内越紧密，类间距离越小，质量越高。


4.  超参数 k 如何选择
====

1.  业务分析法：    根据业务，比如希望把用户分成 高中低 三种。
2.  观察法：    仅适用于低维度的数据。
3.  手肘法 和 轮廓系数法。
4. Gap Statistic。


5.  K-means 算法的优缺点
====

优点：

    1.  算法简单，容易理解，某些情况下聚类效果不错。
    2.  处理大数据集的时候，可以保证较好的伸缩性。
    3.  当数据簇近似于高斯分布时，效果不错。
    4.  算法复杂度低时：时间复杂度：O(nkt)。n 为样本数，k 为类别数，t 为迭代数。

缺点：

    1.  k 值需要人为设定，不同的 k 值得到的结果不一样。
    2.  对初始的类别中心敏感，不同选取方法得到的结果不一样。
    3.  对异常值敏感。
    4.  一个样本只能归为一类，不适合做单样本多标签分类任务。
    5.  不适合
        - 过于离散的分类
        - 样本类别不平衡的分类
        - 非凸形状的分类 (凸形状：同一类别样本分布的形状是凸的，内部是凸集)


6.  简述 SVM 原理
====
SVM(supporting vector machine): 二分类模型。
基本思想：是在特征空间中寻找间隔最大的分离超平面使数据得到高效的二分类。



7.  SVM 为什么采用间隔最大化
====
当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。
感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。
线性可分支持向量机利用间隔最大化求得最优分离超平面，且此时解是唯一的。

参考文献：

    1.  (https://zhuanlan.zhihu.com/p/104355127)
    2.  (https://blog.csdn.net/weixin_44507034/article/details/110010543)
