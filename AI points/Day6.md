**关键词：**
>


## SVM 算法的优缺点
SVM 算法，分类模型的
优点：
    - 能够有效解决高维特征的分类问题和回归问题，在特征维度大于样本数是有很好的效果。
    - 无需依赖全部数据，只使用一部分支持向量来做超平面的决策。
    - 有大量的核函数可以使用，从而可以很灵活的来解决各种非线性的分类回归问题。
    - 分类准确率高，泛化能力强，前提是样本量不是海量数据。

缺点：
    - 如果特征为度 远远大于 样本数，则 SVM 表现一般。
    - SVM 在样本量非常大，核函数映射维度非常高时，计算量过大，效果不好。
    - 非线性问题的核函数的选择没有通用标准，难以选择一个合适的核函数。
    - SVM 对缺失数据敏感。

## SVM 的超参数 C 如何调节
SVM 模型有两个非常重要的参数：

    1.  超参数 C ✅
    2.  Gamma

超参数 C：惩罚系数，就是对误差的宽容度。 

    C 越大，对于训练集的表现越好，但是间隔见效也就是对于噪声等误差的容忍度越小，会出现过拟合，说明对于噪声的惩罚力度太大；   
    C 越小，容易欠拟合。   
    C 过大过小，泛化能力变差。  

- C 趋于 ∞ 时：说明不允许出现分类误差的样本存在，是一个 hard-margin 问题
- C 趋于 0 时：说明不再关注分类是否正确，只要求间隔越大越好，将无法得到有意义的解且算法不会收敛。


## SVM 的核函数如何选择

核函数的类型：
    - 线性内核(Linear Kernel)
    - 多项式内核(Polynomial Kernal)
    - 高斯内核(RBF/径向基)
    - Sigmoid 核

如何选择：
    - 如果 Feature 的数量很大，跟样本数量差不多，这时候选用 LR 或者是 Linear Kernel 的 SVM
    - 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用 SVM + Gaussian Kernel
    - 如果 Feature 的数量比较小，而样本数量很多，需要手工添加一些 feature 变成第一种情况

## SVM 硬间隔推导过程

## SVM 软间隔推导过程

## 决策树的构建过程

## ID3 决策树与 C4.5 决策树的区别 
